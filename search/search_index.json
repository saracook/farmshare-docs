{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to FarmShare \u00b6 FarmShare is Stanford\u2019s community computing environment. It is intended for use in coursework and unsponsored research. It is not approved for use with high-risk data, or for use in sponsored research. FarmShare evolved from the old, public UNIX cluster, once located on the second floor of Sweet Hall, which was itself a successor to systems like the University\u2019s original timeshare service, LOTS. The latest iteration of FarmShare with hardware and major OS update came online early 2024. Resources on FarmShare are focused on making it easier to learn how to use research computing including \u201cscheduler\u201d or \u201cdistributed resource management system\u201d to submit compute jobs. By using FarmShare, new researchers can more easily adapt to using larger clusters when they have big projects that involve using federally funded resources, shared Stanford clusters, or on a small grant funded cluster. Full SUNet (or sponsorship) required. What\u2019s New \u00b6 Key changes on the new FarmShare environment include: Major OS upgrade to Ubuntu 22.04 LTS which brings many changes, improvements and provides LTS stability. New Hardware with CPU and memory improvements. New browser based access with Open OnDemand v3 . Updated Open Ondemand apps: JupyterLabs, RStudio, and VS Code. Updated Scheduler (resource manager) to Slurm v24.11 Home directory path updated to /home/users/USER . Please use the variable $HOME instead of hard coding directory paths. Cluster Components \u00b6 FarmShare consists of three classes of servers: Login nodes called rice servers where you log in to run commands, access files, submit jobs, and review results. The rice servers also have access to Stanford AFS . These servers can be accessed via ssh and can be used for interactive work. Some resource limits are enforced, so if you need to run a long-running or compute- and/or memory-intensive process you should submit a job Compute nodes called iron , rye and wheat servers. They have more CPU power and more memory than the rice servers, and are meant for both interactive jobs (where you log in to control what happens) and batch jobs (where everything runs from a script that you submit) GPU compute nodes called oat servers. They are similar to the compute nodes mentioned above, except that they also have GPUs installed FarmShare currently has 4 rice servers (login nodes) along with 26 compute nodes (iron, rye, wheat, oat nodes). User storage for $HOME along with global /scratch space is available on all nodes. Info All FarmShare servers run Ubuntu 22.04 LTS and get patched/updated on a regular basis Eligibility \u00b6 FarmShare is available to anyone who has a full-service SUNetID. A full-service SUNetID is one that has email service; if you can successfully get to Stanford Webmail , then you are eligible to use FarmShare for academic work. If you do not already have a full-service SUNetID (maybe because you are a visiting researcher), you can get a sponsored full-service SUNetID. Read more about SUNetID levels . Note that, in order to get a sponsored SUNetID, a monthly fee will be charged by University IT. Only people with spending authority may sponsor a SUNetID. Sponsorships can be obtained and paid for through Sponsorship Manager . Current rates are available from the Sponsored Account Rates page . FarmShare is meant for low- or moderate-risk data , and is primarily intended for coursework or research purposes. It is not meant for sponsored research (where you have a dedicated source of funding), and is not approved for handling high-risk data. Important See our Policy section for details Sponsored Research \u00b6 If you are doing sponsored or departmental research, then FarmShare might not be the right place for you. Instead, if the data you are working with is all low-risk, then you should consider our Sherlock Cluster. If you are working with high-risk data then you should consider our Nero GCP or Carina Computing platform. If you are working with complex AI you should consider our Marlowe Cluster. Getting Help \u00b6 Most FarmShare support is provided during business hours, either via email or during academic-year office hours. For email support, send a message to srcc-support . Make sure you have \u201cFarmShare\u201d somewhere in the subject line, and please be as detailed as possible with your request.","title":"Introduction"},{"location":"#welcome-to-farmshare","text":"FarmShare is Stanford\u2019s community computing environment. It is intended for use in coursework and unsponsored research. It is not approved for use with high-risk data, or for use in sponsored research. FarmShare evolved from the old, public UNIX cluster, once located on the second floor of Sweet Hall, which was itself a successor to systems like the University\u2019s original timeshare service, LOTS. The latest iteration of FarmShare with hardware and major OS update came online early 2024. Resources on FarmShare are focused on making it easier to learn how to use research computing including \u201cscheduler\u201d or \u201cdistributed resource management system\u201d to submit compute jobs. By using FarmShare, new researchers can more easily adapt to using larger clusters when they have big projects that involve using federally funded resources, shared Stanford clusters, or on a small grant funded cluster. Full SUNet (or sponsorship) required.","title":"Welcome to FarmShare"},{"location":"#whats-new","text":"Key changes on the new FarmShare environment include: Major OS upgrade to Ubuntu 22.04 LTS which brings many changes, improvements and provides LTS stability. New Hardware with CPU and memory improvements. New browser based access with Open OnDemand v3 . Updated Open Ondemand apps: JupyterLabs, RStudio, and VS Code. Updated Scheduler (resource manager) to Slurm v24.11 Home directory path updated to /home/users/USER . Please use the variable $HOME instead of hard coding directory paths.","title":"What&rsquo;s New"},{"location":"#cluster-components","text":"FarmShare consists of three classes of servers: Login nodes called rice servers where you log in to run commands, access files, submit jobs, and review results. The rice servers also have access to Stanford AFS . These servers can be accessed via ssh and can be used for interactive work. Some resource limits are enforced, so if you need to run a long-running or compute- and/or memory-intensive process you should submit a job Compute nodes called iron , rye and wheat servers. They have more CPU power and more memory than the rice servers, and are meant for both interactive jobs (where you log in to control what happens) and batch jobs (where everything runs from a script that you submit) GPU compute nodes called oat servers. They are similar to the compute nodes mentioned above, except that they also have GPUs installed FarmShare currently has 4 rice servers (login nodes) along with 26 compute nodes (iron, rye, wheat, oat nodes). User storage for $HOME along with global /scratch space is available on all nodes. Info All FarmShare servers run Ubuntu 22.04 LTS and get patched/updated on a regular basis","title":"Cluster Components"},{"location":"#eligibility","text":"FarmShare is available to anyone who has a full-service SUNetID. A full-service SUNetID is one that has email service; if you can successfully get to Stanford Webmail , then you are eligible to use FarmShare for academic work. If you do not already have a full-service SUNetID (maybe because you are a visiting researcher), you can get a sponsored full-service SUNetID. Read more about SUNetID levels . Note that, in order to get a sponsored SUNetID, a monthly fee will be charged by University IT. Only people with spending authority may sponsor a SUNetID. Sponsorships can be obtained and paid for through Sponsorship Manager . Current rates are available from the Sponsored Account Rates page . FarmShare is meant for low- or moderate-risk data , and is primarily intended for coursework or research purposes. It is not meant for sponsored research (where you have a dedicated source of funding), and is not approved for handling high-risk data. Important See our Policy section for details","title":"Eligibility"},{"location":"#sponsored-research","text":"If you are doing sponsored or departmental research, then FarmShare might not be the right place for you. Instead, if the data you are working with is all low-risk, then you should consider our Sherlock Cluster. If you are working with high-risk data then you should consider our Nero GCP or Carina Computing platform. If you are working with complex AI you should consider our Marlowe Cluster.","title":"Sponsored Research"},{"location":"#getting-help","text":"Most FarmShare support is provided during business hours, either via email or during academic-year office hours. For email support, send a message to srcc-support . Make sure you have \u201cFarmShare\u201d somewhere in the subject line, and please be as detailed as possible with your request.","title":"Getting Help"},{"location":"capacity/","text":"Adding Capacity \u00b6 FarmShare does not have a dedicated funding source available to it, and we appreciate any contributions that people can make. For example: If you are using FarmShare for a class, let your department chair know that you are using it. If you are a registered student group using FarmShare, let your faculty representative and/or ASSU know that you are using it. If you have spare funds available, email us and we can arrange either a iJournals transfer to our PTA, or a capital purchase from your PTA. If you already have hardware being supported by SRCC, we may be able to use that hardware once you are done with it. We appreciate anything you can do to get the word out about FarmShare and how awesome it is!","title":"Adding Capacity"},{"location":"capacity/#adding-capacity","text":"FarmShare does not have a dedicated funding source available to it, and we appreciate any contributions that people can make. For example: If you are using FarmShare for a class, let your department chair know that you are using it. If you are a registered student group using FarmShare, let your faculty representative and/or ASSU know that you are using it. If you have spare funds available, email us and we can arrange either a iJournals transfer to our PTA, or a capital purchase from your PTA. If you already have hardware being supported by SRCC, we may be able to use that hardware once you are done with it. We appreciate anything you can do to get the word out about FarmShare and how awesome it is!","title":"Adding Capacity"},{"location":"connecting/","text":"Getting Connected \u00b6 SSH \u00b6 SSH provides a secure, remote terminal connection to the login nodes. There are a number of rice servers providing login service (e.g., rice-01, rice-02); the easiest way to connect is using the load-balanced name, login.farmshare.stanford.edu , which will select one for you according to recent utilization. Depending on your environment and configuration you may be prompted for your SUNet ID password, and you\u2019ll need to complete your login using two-step authentication . Users on Linux, macOS, and Windows can use the built-in Terminal application and run ssh . Replace SUNetID with your own SUNet ID: Example ssh SUNetID@login.farmshare.stanford.edu The example below shows how to login in using your SUNet ID and password along with two-step authentication. To logout, type exit or logout : $ ssh ta5@login.farmshare.stanford.edu ta5@login.farmshare.stanford.edu's password: (ta5@login.farmshare.stanford.edu) Duo two-factor login for ta5 Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-XXXX 2. Phone call to XXX-XXX-XXXX 3. SMS passcodes to XXX-XXX-XXXX Passcode or option (1-3): 1 Success. Logging you in... Success. Logging you in... Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-50-generic x86_64) Stanford Research Computing (https://srcc.stanford.edu/) ----------------------- FarmShare (https://docs.farmshare.stanford.edu/) ... ta5@rice-02:~$ ta5@rice-02:~$ ta5@rice-02:~$ exit logout Shared connection to login.farmshare.stanford.edu closed. Host keys \u00b6 For those who wish to verify the new FarmShare host keys when connecting, use the following fingerprints: SHA256:bKb1Znir/1tOg+TMyALDYWeK0lclsulriDN8aOvWteU (ED25519) SHA256:o5E5OOkaxwF+CzKT5A2/DNSmDzljTYs/a1V7Fm6ssSw (RSA) Other SSH clients \u00b6 PuTTY \u00b6 PuTTY is a popular, freely available SSH client for Windows. The default settings are appropriate for most users, so all you need to do is specify the host name and click the Open button to connect. MobaXterm \u00b6 MobaXterm is an SSH client with a built-in X server, making remote display extremely convenient, and the Home Edition is free for personal use. Mobile Shell (Mosh) \u00b6 Mosh is an alternative to SSH for Linux and macOS clients. It uses SSH for authentication, so you may want to review the suggested SSH configuration above. Mosh has some advantages, including predictive display, which can be useful on high-latency connections, and improved network resiliency. Mosh connections can persist when you switch networks and can even survive putting your computer to sleep. Open OnDemand \u00b6 Open OnDemand is a browser based interface to FarmShare. Open OnDemand offers terminal, file manager, editor, desktop, and GUI applications right from your web browser! Logging in \u00b6 To use the FarmShare OnDemand interface: Connect to https://ondemand.farmshare.stanford.edu Use your SUNetID credentials and go through the two-step authentication process Note If you have never logged into Farmshare before you won\u2019t be able to run apps/jobs until your Slurm account is created. This is done when you first connect to a login node. You can do that from Ondemand by selecting the Clusters drop down > FarmShare Shell Access .**","title":"Getting Connected"},{"location":"connecting/#getting-connected","text":"","title":"Getting Connected"},{"location":"connecting/#ssh","text":"SSH provides a secure, remote terminal connection to the login nodes. There are a number of rice servers providing login service (e.g., rice-01, rice-02); the easiest way to connect is using the load-balanced name, login.farmshare.stanford.edu , which will select one for you according to recent utilization. Depending on your environment and configuration you may be prompted for your SUNet ID password, and you\u2019ll need to complete your login using two-step authentication . Users on Linux, macOS, and Windows can use the built-in Terminal application and run ssh . Replace SUNetID with your own SUNet ID: Example ssh SUNetID@login.farmshare.stanford.edu The example below shows how to login in using your SUNet ID and password along with two-step authentication. To logout, type exit or logout : $ ssh ta5@login.farmshare.stanford.edu ta5@login.farmshare.stanford.edu's password: (ta5@login.farmshare.stanford.edu) Duo two-factor login for ta5 Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-XXXX 2. Phone call to XXX-XXX-XXXX 3. SMS passcodes to XXX-XXX-XXXX Passcode or option (1-3): 1 Success. Logging you in... Success. Logging you in... Welcome to Ubuntu 22.04.5 LTS (GNU/Linux 6.8.0-50-generic x86_64) Stanford Research Computing (https://srcc.stanford.edu/) ----------------------- FarmShare (https://docs.farmshare.stanford.edu/) ... ta5@rice-02:~$ ta5@rice-02:~$ ta5@rice-02:~$ exit logout Shared connection to login.farmshare.stanford.edu closed.","title":"SSH"},{"location":"connecting/#host-keys","text":"For those who wish to verify the new FarmShare host keys when connecting, use the following fingerprints: SHA256:bKb1Znir/1tOg+TMyALDYWeK0lclsulriDN8aOvWteU (ED25519) SHA256:o5E5OOkaxwF+CzKT5A2/DNSmDzljTYs/a1V7Fm6ssSw (RSA)","title":"Host keys"},{"location":"connecting/#other-ssh-clients","text":"","title":"Other SSH clients"},{"location":"connecting/#putty","text":"PuTTY is a popular, freely available SSH client for Windows. The default settings are appropriate for most users, so all you need to do is specify the host name and click the Open button to connect.","title":"PuTTY"},{"location":"connecting/#mobaxterm","text":"MobaXterm is an SSH client with a built-in X server, making remote display extremely convenient, and the Home Edition is free for personal use.","title":"MobaXterm"},{"location":"connecting/#mobile-shell-mosh","text":"Mosh is an alternative to SSH for Linux and macOS clients. It uses SSH for authentication, so you may want to review the suggested SSH configuration above. Mosh has some advantages, including predictive display, which can be useful on high-latency connections, and improved network resiliency. Mosh connections can persist when you switch networks and can even survive putting your computer to sleep.","title":"Mobile Shell (Mosh)"},{"location":"connecting/#open-ondemand","text":"Open OnDemand is a browser based interface to FarmShare. Open OnDemand offers terminal, file manager, editor, desktop, and GUI applications right from your web browser!","title":"Open OnDemand"},{"location":"connecting/#logging-in","text":"To use the FarmShare OnDemand interface: Connect to https://ondemand.farmshare.stanford.edu Use your SUNetID credentials and go through the two-step authentication process Note If you have never logged into Farmshare before you won\u2019t be able to run apps/jobs until your Slurm account is created. This is done when you first connect to a login node. You can do that from Ondemand by selecting the Clusters drop down > FarmShare Shell Access .**","title":"Logging in"},{"location":"policy/","text":"Policy \u00b6 Authorized Users \u00b6 Stanford students, faculty, and staff, and other users with a full-service SUNet ID , may use FarmShare resources. Acceptable Use \u00b6 FarmShare is intended for use in coursework and unsponsored research, and is subject to University policies on acceptable use and standards of behavior. These include, but are not limited to, policies in the Administrative Guide (especially the University Code of Conduct and Computer and Network Usage Policy ) and the Research Policy Handbook , the Honor Code , and the Fundamental Standard . Risk Classification \u00b6 FarmShare is not approved for use with high-risk data, including protected health information and personally identifiable information. Do not use FarmShare to transmit, store, or process high-risk data.","title":"Policy"},{"location":"policy/#policy","text":"","title":"Policy"},{"location":"policy/#authorized-users","text":"Stanford students, faculty, and staff, and other users with a full-service SUNet ID , may use FarmShare resources.","title":"Authorized Users"},{"location":"policy/#acceptable-use","text":"FarmShare is intended for use in coursework and unsponsored research, and is subject to University policies on acceptable use and standards of behavior. These include, but are not limited to, policies in the Administrative Guide (especially the University Code of Conduct and Computer and Network Usage Policy ) and the Research Policy Handbook , the Honor Code , and the Fundamental Standard .","title":"Acceptable Use"},{"location":"policy/#risk-classification","text":"FarmShare is not approved for use with high-risk data, including protected health information and personally identifiable information. Do not use FarmShare to transmit, store, or process high-risk data.","title":"Risk Classification"},{"location":"slurm/","text":"Running Jobs \u00b6 Slurm \u00b6 FarmShare uses Slurm for job (resource) management. Full documentation and detailed usage information is provided in the man pages for the srun, sbatch, squeue, scancel, sinfo, and scontrol commands. Jobs are scheduled according to a priority which depends on a number of factors, including how long a job has been waiting, its size, and a fair-share value that tracks recent per-user utilization of cluster resources. Lower-priority jobs, and jobs requiring access to resources not currently available, may wait some time before starting to run. The scheduler may reserve resources so that pending jobs can start; while it will try to backfill these resources with smaller, shorter jobs (even those at lower priorities), this behavior can sometimes cause nodes to appear to be idle even when there are jobs that are ready to run. You can use squeue --start to get an estimate of when pending jobs will start. Slurm commands \u00b6 Slurm allows requesting resources and submitting jobs in a variety of ways. The main Slurm commands to submit jobs are listed in the table below: Command Description Behavior salloc Request resources and allocates them to a job Starts a new shell, but does not execute anything srun Request resources and runs a command on the allocated compute node(s) Execute command on compute node sbatch Request resources and runs a script on the allocated compute node(s) Submit a batch script to Slurm squeue View job and job step information Displays job information scancel Signal or cancel jobs, job arrays or job steps Cancel running job sinfo View information about Slurm nodes and partitions Displays partition information Interactive Jobs \u00b6 Interactive sessions that require resources in excess of limits on the login nodes, exclusive access to resources, or access to a feature not available on the login nodes (e.g., a GPU), can be submitted to a compute node. Each user is allowed one interactive job, which may run for at most one day. You can use the srun command to request one: ta5@rice-04:~$ srun --qos=interactive --pty bash ta5@wheat-01:~$ Batch Jobs \u00b6 The sbatch command is used to submit a batch job. A job is simply an instance of your program, for example your R, Python or Matlab script that is submitted to and executed by the scheduler (Slurm). When you submit a job with the sbatch command it\u2019s called a batch job and it will either run immediately or will pend (wait) in the queue. Options are used to request specific resources (including runtime), and can be provided either on the command line or, using a special syntax, in the script file itself. sbatch can also be used to submit many similar jobs, each perhaps varying in only one or two parameters, in a single invocation using the \u2013array option; each job in an array has access to environment variables identifying its rank. CPUs: How many CPUs the program you are calling the in the sbatch script needs, unless it can utilize multiple CPUs at once you should request a single CPU. Check your code\u2019s documentation or try running in an interactive session with and run htop if you are unsure. memory (RAM): How much memory your job will consume. Some things to consider, will it load a large file or matrix into memory? Does it consume a lot of memory on your laptop? Often the default memory is sufficient for many jobs. time: How long will it take for your code to run to completion? partition: What set of compute nodes on FarmShare will you run on, normal, interactive, bigmem? The default partition on FarmShare is the normal partition. Sample batch script to submit a job: ta5@rice-04:~$ cat hello_world.sh #!/bin/bash #SBATCH --job-name=hello_world #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --partition=normal echo 'Hello World!' ta5@rice-04:~$ ta5@rice-04:~$ ta5@rice-04:~$ sbatch hello_world.sh Submitted batch job 177987 ta5@rice-04:~$ ta5@rice-04:~$ cat slurm-177987.out Hello World! ta5@rice-04:~$ Partition/QoS Info \u00b6 FarmShare provides the following partitions and QoS : ta5@rice-02:~$ sacctmgr show qos format=name%11,maxsubmitjobspu,maxjobspu,mintres%10,maxtrespu%25,maxwall Name MaxSubmitPU MaxJobsPU MinTRES MaxTRESPU MaxWall ----------- ----------- --------- ---------- ------------------------- ----------- normal 1024 128 cpu=256,gres/gpu=3 interactive 3 3 cpu=16,gres/gpu=1,mem=64G dev 1 1 cpu=8,gres/gpu=1,mem=32G 08:00:00 long 32 4 cpu=32 7-00:00:00 caddyshack 1 1 cpu=8,mem=32G bigmem 32 4 mem=192G mem=768G gpu 32 4 gres/gpu=1 gres/gpu=6 Partition Max Memory Max CPU normal 188GB 256 bigmem 768GB 344 interactive 188GB 16","title":"Running Jobs"},{"location":"slurm/#running-jobs","text":"","title":"Running Jobs"},{"location":"slurm/#slurm","text":"FarmShare uses Slurm for job (resource) management. Full documentation and detailed usage information is provided in the man pages for the srun, sbatch, squeue, scancel, sinfo, and scontrol commands. Jobs are scheduled according to a priority which depends on a number of factors, including how long a job has been waiting, its size, and a fair-share value that tracks recent per-user utilization of cluster resources. Lower-priority jobs, and jobs requiring access to resources not currently available, may wait some time before starting to run. The scheduler may reserve resources so that pending jobs can start; while it will try to backfill these resources with smaller, shorter jobs (even those at lower priorities), this behavior can sometimes cause nodes to appear to be idle even when there are jobs that are ready to run. You can use squeue --start to get an estimate of when pending jobs will start.","title":"Slurm"},{"location":"slurm/#slurm-commands","text":"Slurm allows requesting resources and submitting jobs in a variety of ways. The main Slurm commands to submit jobs are listed in the table below: Command Description Behavior salloc Request resources and allocates them to a job Starts a new shell, but does not execute anything srun Request resources and runs a command on the allocated compute node(s) Execute command on compute node sbatch Request resources and runs a script on the allocated compute node(s) Submit a batch script to Slurm squeue View job and job step information Displays job information scancel Signal or cancel jobs, job arrays or job steps Cancel running job sinfo View information about Slurm nodes and partitions Displays partition information","title":"Slurm commands"},{"location":"slurm/#interactive-jobs","text":"Interactive sessions that require resources in excess of limits on the login nodes, exclusive access to resources, or access to a feature not available on the login nodes (e.g., a GPU), can be submitted to a compute node. Each user is allowed one interactive job, which may run for at most one day. You can use the srun command to request one: ta5@rice-04:~$ srun --qos=interactive --pty bash ta5@wheat-01:~$","title":"Interactive Jobs"},{"location":"slurm/#batch-jobs","text":"The sbatch command is used to submit a batch job. A job is simply an instance of your program, for example your R, Python or Matlab script that is submitted to and executed by the scheduler (Slurm). When you submit a job with the sbatch command it\u2019s called a batch job and it will either run immediately or will pend (wait) in the queue. Options are used to request specific resources (including runtime), and can be provided either on the command line or, using a special syntax, in the script file itself. sbatch can also be used to submit many similar jobs, each perhaps varying in only one or two parameters, in a single invocation using the \u2013array option; each job in an array has access to environment variables identifying its rank. CPUs: How many CPUs the program you are calling the in the sbatch script needs, unless it can utilize multiple CPUs at once you should request a single CPU. Check your code\u2019s documentation or try running in an interactive session with and run htop if you are unsure. memory (RAM): How much memory your job will consume. Some things to consider, will it load a large file or matrix into memory? Does it consume a lot of memory on your laptop? Often the default memory is sufficient for many jobs. time: How long will it take for your code to run to completion? partition: What set of compute nodes on FarmShare will you run on, normal, interactive, bigmem? The default partition on FarmShare is the normal partition. Sample batch script to submit a job: ta5@rice-04:~$ cat hello_world.sh #!/bin/bash #SBATCH --job-name=hello_world #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --partition=normal echo 'Hello World!' ta5@rice-04:~$ ta5@rice-04:~$ ta5@rice-04:~$ sbatch hello_world.sh Submitted batch job 177987 ta5@rice-04:~$ ta5@rice-04:~$ cat slurm-177987.out Hello World! ta5@rice-04:~$","title":"Batch Jobs"},{"location":"slurm/#partitionqos-info","text":"FarmShare provides the following partitions and QoS : ta5@rice-02:~$ sacctmgr show qos format=name%11,maxsubmitjobspu,maxjobspu,mintres%10,maxtrespu%25,maxwall Name MaxSubmitPU MaxJobsPU MinTRES MaxTRESPU MaxWall ----------- ----------- --------- ---------- ------------------------- ----------- normal 1024 128 cpu=256,gres/gpu=3 interactive 3 3 cpu=16,gres/gpu=1,mem=64G dev 1 1 cpu=8,gres/gpu=1,mem=32G 08:00:00 long 32 4 cpu=32 7-00:00:00 caddyshack 1 1 cpu=8,mem=32G bigmem 32 4 mem=192G mem=768G gpu 32 4 gres/gpu=1 gres/gpu=6 Partition Max Memory Max CPU normal 188GB 256 bigmem 768GB 344 interactive 188GB 16","title":"Partition/QoS Info"},{"location":"software/","text":"Using Software \u00b6 FarmShare has lots of free and commercial software available to use. We provide software that comes with Ubuntu, software that we package ourselves, and we also provide the capability for you to build and use software yourself. Software Sources \u00b6 Software on FarmShare comes from three sources: Packages Modules Build your own Packages \u00b6 Packaged software is easiest to use, because you don\u2019t have to do anything. Packaged software has already been installed on all of the systems in the environment, so to use the software, you just have to run the command. For example, to run the packaged version of Python, you just need to run the python3 command: ta5@rice-01:~$ python3 --version Python 3.10.12 Note FarmShare runs Ubuntu 22.04 LTS, which means almost anything in the Ubuntu Jammy package repository is available to be installed. Modules \u00b6 To provide up-to-date software, FarmShare uses loadable environment modules . Modularized software lives in shared, network-connected storage, and is built by the FarmShare support team. This mechanism allows us to provide multiple versions of the same software concurrently, and gives users the possibility to easily switch between software versions. The module system is used to manage the user environment and to activate software packages on demand. In order to use modules based software installed on FarmShare, you must first load the corresponding software module. When you load a module, the system will set or modify your user environment variables to enable access to the software package provided by that module. For instance, the $PATH environment variable might be updated so that appropriate executable for that package can be used. The example below shows how to load r module for the R software: ta5@rice-04:~$ module load r ta5@rice-04:~$ R --version R version 4.4.0 (2024-04-24) -- \"Puppy Cup\" Copyright (C) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/. Module usage \u00b6 The most common module commands are outlined in the following table. module commands may be shortened with the ml alias. Module command Short version Description module avail ml av List available software module spider r ml spider r Search for particular software module keyword blas ml key blas Search for blas in module names and descriptions module whatis gcc ml whatis gcc Display information about the gcc module module help gcc ml help gcc Display module specific help module load gcc ml gcc Load a module to use the associated software module load r/4.3.3 ml r/4.3.3 Load specific version of a module module unload gcc ml -gcc Unload a module module purge ml purge Remove all modules module save foo ml save foo Save the state of all loaded modules in a collection named foo module restore foo ml restore foo Restore the state of saved modules from the foo collection Info Additional module sub-commands are documented in the module help command. For complete reference, please refer to the official Lmod documentation Module properties \u00b6 To quickly see some of the modules characteristics, module avail will display colored property attributes next to the module names. The main module properties are: S : Module is sticky, requires --force to unload or purge L : Indicate currently loaded module D : Default module that will be loaded when multiple versions are available g : GPU-accelerated software, will only run on GPU nodes Searching for modules \u00b6 You can search through all the available modules for either: a module name (if you already know it), using module spider any string within modules names and descriptions, using module keyword For instance, if you want to know how to load the apptainer module, you can do: ta5@rice-04:~$ module spider apptainer If you don\u2019t know the module name, or want to list all the modules that contain a specific string of characters in their name or description, you can use module keyword . For instance, the following command will list all the modules providing a BLAS library: ta5@rice-04:~$ module keyword blas Listing \u00b6 For a complete list of available software modules, run the module available command: ta5@rice-01:~$ module available ---------------------------------------------- /software/modules/linux-ubuntu22.04-x86_64/Core ----------------------------------------------- apptainer/1.1.9 intel-oneapi-compilers/2024.1.0 micromamba/1.4.2 python/3.11.7 apptainer/1.3.4 (D) intel-oneapi-dal/2024.2.0 mpich/4.2.1 python/3.12.5 blast-plus/2.14.1 intel-oneapi-dnn/2024.1.1 ncurses/5.9 python/3.13.0 (D) boost/1.85.0 intel-oneapi-dpl/2022.5.0 openblas/0.3.26 r-magick/2.7.4_r/4.3.3 bowtie2/2.5.2 intel-oneapi-ipp/2021.11.0 openmpi/5.0.3 r-magick/2.7.4_r/4.4.0 (D) cuda/11.4.4 intel-oneapi-ippcp/2021.11.0 pandoc/2.19.2 r-tidyverse/2.0.0_r/4.3.3 cudnn/8.2.4.15-11.4 intel-oneapi-mkl/2024.0.0 paraview/5.12.0 r/4.3.3 fastqc/0.12.1 intel-oneapi-mpi/2021.12.1 postgresql/15.2 r/4.4.0 (D) gcc/13.2.0 intel-oneapi-tbb/2021.12.0 py-pip/23.1.2_python/3.10.13 rust/1.78.0 gcc/14.2.0 (D) julia/1.9.3 py-pip/23.1.2_python/3.11.7 texlive/20240312 ghostscript/10.0.0 julia/1.10.2 py-pip/23.1.2_python/3.12.5 imagemagick/7.1.1-29 julia/1.11.0 (D) py-pip/23.1.2_python/3.13.0 (D) intel-oneapi-ccl/2021.12.0 llvm/18.1.3 python/3.10.13 -------------------------------------------------------- /software/modules/commercial -------------------------------------------------------- ansys/2024r2 gurobi/12.0.0 mathematica/14.1.0 (D) sas/9.4m8 stata/18 gaussian/g16-a.03 mathematica/13.3.1 matlab/r2023b (D) schrodinger/2024-4 (g) gaussian/g16-b.01 (D) mathematica/14.0.0 matlab/r2024a stata/now (D) Build Your Own \u00b6 In addition to the software that we provide for you to use, it is perfectly OK for you to build and use your own software. Python \u00b6 Different versions of Python3 are available on FarmShare both as system modules as well as system software. To see a listing run module spider python We install commonly used Python packages (such as NumPy, SciPy) globally that are available when you load Python with the module command. To install packages that are not already installed you can use pip pip \u00b6 pip is available as a module on FarmShare. You will need to use the --user flag which places the package installation under your $HOME directory. The example below shows how to install pandas : ta5@rice-02:~$ module load py-pip/23.1.2_python/3.13.0 ta5@rice-02:~$ module load python/3.13.0 ta5@rice-02:~$ ta5@rice-02:~$ python --version Python 3.13.0 ta5@rice-02:~$ ta5@rice-02:~$ python3 -m pip install --user pandas ... ta5@rice-02:~$ python3 -m pip freeze numpy==2.2.2 pandas==2.2.3 python-dateutil==2.9.0.post0 pytz==2024.2 six==1.17.0 tzdata==2025.1 Virtual Environments \u00b6 Virtual environment is an isolated space for your Python projects, allowing you to manage dependencies separately for each project. You can create a personal Python environment that will persist each time you log in. There is no risk of packages being updated and allows greater control over your environment. To create python virtual environments, start by loading your preferred version of Python and use the venv command: ta5@rice-02:~$ module load python/3.13.0 ta5@rice-02:~$ ta5@rice-02:~$ python3 -m venv tutorial_env ta5@rice-02:~$ source tutorial_env/bin/activate (tutorial_env) ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ python --version Python 3.13.0 (tutorial_env) ta5@rice-02:~$ This will create a new virtual environment in the tutorial_env (the name inside the parentheses) subdirectory, and configure the current shell to use it as the default python environment. Here you can install packages with pip : (tutorial_env) ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ pip install pandas Installing setuptools and wheel projects are useful to ensure you can also install from source archives: (tutorial_env) ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ pip install --upgrade pip setuptools wheel (tutorial_env) ta5@rice-02:~$ pip freeze numpy==2.2.2 pandas==2.2.3 python-dateutil==2.9.0.post0 pytz==2024.2 setuptools==75.8.0 six==1.17.0 tzdata==2025.1 wheel==0.45.1 To deactivate or leave the environment tutorial_env : (tutorial_env) ta5@rice-02:~$ deactivate Virtual Environment in Slurm \u00b6 Python virtual environments can be used in slurm jobs. To submit a sbatch job using a venv environment, you can source the environment at the top of the sbatch script. Sample python script that prints versions of packages: ta5@rice-02:~$ cat test.py import numpy as np import pandas as pd import sys print(f\"Python version = {sys.version}\") print(f\"Numpy version = {np.version.version}\") print(f\"Pandas version = {pd.__version__}\") To submit this script using the venv tutorial_env create a sbatch script to load the venv and run test.py : ta5@rice-02:~$ cat tutorial_env.sh #!/bin/bash #SBATCH --job-name=tutorial_env #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --partition=normal # Load venv tutorial_env source tutorial_env/bin/activate # Run script python3 test.py ta5@rice-02:~$ ta5@rice-02:~$ ta5@rice-02:~$ sbatch tutorial_env.sh Submitted batch job 298438 ta5@rice-02:~$ cat slurm-298438.out Python version = 3.13.0 (main, Dec 10 2024, 13:22:44) [GCC 13.2.0] Numpy version = 2.2.2 Pandas version = 2.2.3 JupyterLab \u00b6 JupyterLab is Project Jupyter\u2019s web-based development interface for Jupyter Notebooks. On FarmShare, it is available as an app on our OnDemand interface and supports computation with Python 3. Accessing JupyterLab \u00b6 Login in to OnDemand and select Interactive Apps > JupyterLab If you want to make one of your virtual environments available for use in Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment (if you do not have an environment, refer to the sections above on how to do so). ta5@rice-02:~$ source tutorial_env/bin/activate (tutorial_env) ta5@rice-02:~$ pip install ipykernel ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ python3 -m ipykernel install --user --name tutorial_env Installed kernelspec tutorial_env in /home/users/ta5/.local/share/jupyter/kernels/tutorial_env Once you\u2019ve successfully created your kernel, you should see your environment (custom kernel name) at the Notebook Launcher! Apptainer \u00b6 Containers are isolated environments packaged together with an executable so that no additional installation or setup is required for running on any system. Apptainer (formerly known as Singularity), is a container runtime that is available on FarmShare. Apptainer, and Linux containers in general allow sharing pipelines in a portable, reproducible way. You can create and customize your own containers, and because Apptainer also supports Docker containers, you have immediate access to a very large number of Apptainer and Docker containers available via repositories: DockerHub Sylabs Running Apptainer \u00b6 This example will request an interactive session and use the Docker container python/3.13.1-alpine3.21 from DockerHub. This container provides the latest release of python in an Alpine OS environment. The first step is to request an interactive session with multiple cores: ta5@rice-04:~$ srun --partition=interactive --cpus-per-task=4 --qos=interactive --pty bash ta5@iron-06:~$ Next create a directory /scratch/users/$USER/lxd to store all your images. Now load the apptainer module and pull the image: ta5@iron-06:~$ pwd /home/users/ta5 ta5@iron-06:~$ ta5@iron-06:~$ cd /scratch/users/$USER ta5@iron-06:/scratch/users/ta5$ ta5@iron-06:/scratch/users/ta5$ mkdir lxc ta5@iron-06:/scratch/users/ta5$ ta5@iron-06:/scratch/users/ta5$ cd lxc/ ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ module load apptainer ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ apptainer pull docker://python:3.13.1-alpine3.21 INFO: Converting OCI blobs to SIF format INFO: Starting build... Copying blob 2109cea89a77 done | Copying blob 1f3e46996e29 done | Copying blob b7c174cb6c8c done | Copying blob 7486ee1cd0b3 done | Copying config d5cb4e1bd6 done | Writing manifest to image destination 2025/01/29 12:52:06 info unpack layer: sha256:1f3e46996e2966e4faa5846e56e76e3748b7315e2ded61476c24403d592134f0 2025/01/29 12:52:06 info unpack layer: sha256:7486ee1cd0b33ed93151ce1d3f73254a0987b484773adb31f37fe42bad78ba63 2025/01/29 12:52:06 info unpack layer: sha256:b7c174cb6c8cb4276eaff5e9ebfb56eb7124be68c8fcb4518cb5eb6b18245cf5 2025/01/29 12:52:07 info unpack layer: sha256:2109cea89a773ab9365659a6a63a1b7d8be1f7be6031112a429533bd7ba07f68 INFO: Creating SIF file... ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ ls python_3.13.1-alpine3.21.sif ta5@iron-06:/scratch/users/ta5/lxc$ Once the image is downloaded, you can run the container using apptainer run : ta5@iron-06:/scratch/users/ta5/lxc$ apptainer run python_3.13.1-alpine3.21.sif Python 3.13.1 (main, Jan 24 2025, 19:30:15) [GCC 14.2.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> a = (1, 2, 3, 4, 5) >>> x = sum(a) >>> print(x) 15 >>> exit ta5@iron-06:/scratch/users/ta5/lxc$ To launch a shell within the container using apptainer shell : ta5@iron-06:/scratch/users/ta5/lxc$ apptainer shell python_3.13.1-alpine3.21.sif Apptainer> cat /etc/issue Welcome to Alpine Linux 3.21 Kernel \\r on an \\m (\\l) Apptainer> exit ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ cat /etc/issue Ubuntu 22.04.5 LTS \\n \\l Apptainer usage \u00b6 The most common apptainer commands are outlined in the following table. Command Description apptainer run <image> run predefined script within container apptainer exec <image> execute any command within container apptainer shell <image> run bash shell within container Batch job example \u00b6 In the example above Apptainer was running interactively. The example below shows how to run it as a batch job to calculates the sum of one to five: ta5@iron-03:/scratch/users/ta5/lxc$ cat sum.py a = (1, 2, 3, 4, 5) x = sum(a) print(x) To submit this script using apptainer, create a sbatch script to run apptainer exec : ta5@rice-03:/scratch/users/ta5/lxc$ cat ~/tutorial_lxc.sh #!/bin/bash #SBATCH --job-name=tutorial_lxc #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --partition=normal # Load apptainer module load apptainer # Run script with apptainer exec apptainer exec python_3.13.1-alpine3.21.sif python sum.py ta5@rice-03:/scratch/users/ta5/lxc$ ta5@rice-03:/scratch/users/ta5/lxc$ sbatch ~/tutorial_lxc.sh Submitted batch job 299057 ta5@rice-03:/scratch/users/ta5/lxc$ cat slurm-299057.out 15","title":"Using Software"},{"location":"software/#using-software","text":"FarmShare has lots of free and commercial software available to use. We provide software that comes with Ubuntu, software that we package ourselves, and we also provide the capability for you to build and use software yourself.","title":"Using Software"},{"location":"software/#software-sources","text":"Software on FarmShare comes from three sources: Packages Modules Build your own","title":"Software Sources"},{"location":"software/#packages","text":"Packaged software is easiest to use, because you don\u2019t have to do anything. Packaged software has already been installed on all of the systems in the environment, so to use the software, you just have to run the command. For example, to run the packaged version of Python, you just need to run the python3 command: ta5@rice-01:~$ python3 --version Python 3.10.12 Note FarmShare runs Ubuntu 22.04 LTS, which means almost anything in the Ubuntu Jammy package repository is available to be installed.","title":"Packages"},{"location":"software/#modules","text":"To provide up-to-date software, FarmShare uses loadable environment modules . Modularized software lives in shared, network-connected storage, and is built by the FarmShare support team. This mechanism allows us to provide multiple versions of the same software concurrently, and gives users the possibility to easily switch between software versions. The module system is used to manage the user environment and to activate software packages on demand. In order to use modules based software installed on FarmShare, you must first load the corresponding software module. When you load a module, the system will set or modify your user environment variables to enable access to the software package provided by that module. For instance, the $PATH environment variable might be updated so that appropriate executable for that package can be used. The example below shows how to load r module for the R software: ta5@rice-04:~$ module load r ta5@rice-04:~$ R --version R version 4.4.0 (2024-04-24) -- \"Puppy Cup\" Copyright (C) 2024 The R Foundation for Statistical Computing Platform: x86_64-pc-linux-gnu R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under the terms of the GNU General Public License versions 2 or 3. For more information about these matters see https://www.gnu.org/licenses/.","title":"Modules"},{"location":"software/#module-usage","text":"The most common module commands are outlined in the following table. module commands may be shortened with the ml alias. Module command Short version Description module avail ml av List available software module spider r ml spider r Search for particular software module keyword blas ml key blas Search for blas in module names and descriptions module whatis gcc ml whatis gcc Display information about the gcc module module help gcc ml help gcc Display module specific help module load gcc ml gcc Load a module to use the associated software module load r/4.3.3 ml r/4.3.3 Load specific version of a module module unload gcc ml -gcc Unload a module module purge ml purge Remove all modules module save foo ml save foo Save the state of all loaded modules in a collection named foo module restore foo ml restore foo Restore the state of saved modules from the foo collection Info Additional module sub-commands are documented in the module help command. For complete reference, please refer to the official Lmod documentation","title":"Module usage"},{"location":"software/#module-properties","text":"To quickly see some of the modules characteristics, module avail will display colored property attributes next to the module names. The main module properties are: S : Module is sticky, requires --force to unload or purge L : Indicate currently loaded module D : Default module that will be loaded when multiple versions are available g : GPU-accelerated software, will only run on GPU nodes","title":"Module properties"},{"location":"software/#searching-for-modules","text":"You can search through all the available modules for either: a module name (if you already know it), using module spider any string within modules names and descriptions, using module keyword For instance, if you want to know how to load the apptainer module, you can do: ta5@rice-04:~$ module spider apptainer If you don\u2019t know the module name, or want to list all the modules that contain a specific string of characters in their name or description, you can use module keyword . For instance, the following command will list all the modules providing a BLAS library: ta5@rice-04:~$ module keyword blas","title":"Searching for modules"},{"location":"software/#listing","text":"For a complete list of available software modules, run the module available command: ta5@rice-01:~$ module available ---------------------------------------------- /software/modules/linux-ubuntu22.04-x86_64/Core ----------------------------------------------- apptainer/1.1.9 intel-oneapi-compilers/2024.1.0 micromamba/1.4.2 python/3.11.7 apptainer/1.3.4 (D) intel-oneapi-dal/2024.2.0 mpich/4.2.1 python/3.12.5 blast-plus/2.14.1 intel-oneapi-dnn/2024.1.1 ncurses/5.9 python/3.13.0 (D) boost/1.85.0 intel-oneapi-dpl/2022.5.0 openblas/0.3.26 r-magick/2.7.4_r/4.3.3 bowtie2/2.5.2 intel-oneapi-ipp/2021.11.0 openmpi/5.0.3 r-magick/2.7.4_r/4.4.0 (D) cuda/11.4.4 intel-oneapi-ippcp/2021.11.0 pandoc/2.19.2 r-tidyverse/2.0.0_r/4.3.3 cudnn/8.2.4.15-11.4 intel-oneapi-mkl/2024.0.0 paraview/5.12.0 r/4.3.3 fastqc/0.12.1 intel-oneapi-mpi/2021.12.1 postgresql/15.2 r/4.4.0 (D) gcc/13.2.0 intel-oneapi-tbb/2021.12.0 py-pip/23.1.2_python/3.10.13 rust/1.78.0 gcc/14.2.0 (D) julia/1.9.3 py-pip/23.1.2_python/3.11.7 texlive/20240312 ghostscript/10.0.0 julia/1.10.2 py-pip/23.1.2_python/3.12.5 imagemagick/7.1.1-29 julia/1.11.0 (D) py-pip/23.1.2_python/3.13.0 (D) intel-oneapi-ccl/2021.12.0 llvm/18.1.3 python/3.10.13 -------------------------------------------------------- /software/modules/commercial -------------------------------------------------------- ansys/2024r2 gurobi/12.0.0 mathematica/14.1.0 (D) sas/9.4m8 stata/18 gaussian/g16-a.03 mathematica/13.3.1 matlab/r2023b (D) schrodinger/2024-4 (g) gaussian/g16-b.01 (D) mathematica/14.0.0 matlab/r2024a stata/now (D)","title":"Listing"},{"location":"software/#build-your-own","text":"In addition to the software that we provide for you to use, it is perfectly OK for you to build and use your own software.","title":"Build Your Own"},{"location":"software/#python","text":"Different versions of Python3 are available on FarmShare both as system modules as well as system software. To see a listing run module spider python We install commonly used Python packages (such as NumPy, SciPy) globally that are available when you load Python with the module command. To install packages that are not already installed you can use pip","title":"Python"},{"location":"software/#pip","text":"pip is available as a module on FarmShare. You will need to use the --user flag which places the package installation under your $HOME directory. The example below shows how to install pandas : ta5@rice-02:~$ module load py-pip/23.1.2_python/3.13.0 ta5@rice-02:~$ module load python/3.13.0 ta5@rice-02:~$ ta5@rice-02:~$ python --version Python 3.13.0 ta5@rice-02:~$ ta5@rice-02:~$ python3 -m pip install --user pandas ... ta5@rice-02:~$ python3 -m pip freeze numpy==2.2.2 pandas==2.2.3 python-dateutil==2.9.0.post0 pytz==2024.2 six==1.17.0 tzdata==2025.1","title":"pip"},{"location":"software/#virtual-environments","text":"Virtual environment is an isolated space for your Python projects, allowing you to manage dependencies separately for each project. You can create a personal Python environment that will persist each time you log in. There is no risk of packages being updated and allows greater control over your environment. To create python virtual environments, start by loading your preferred version of Python and use the venv command: ta5@rice-02:~$ module load python/3.13.0 ta5@rice-02:~$ ta5@rice-02:~$ python3 -m venv tutorial_env ta5@rice-02:~$ source tutorial_env/bin/activate (tutorial_env) ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ python --version Python 3.13.0 (tutorial_env) ta5@rice-02:~$ This will create a new virtual environment in the tutorial_env (the name inside the parentheses) subdirectory, and configure the current shell to use it as the default python environment. Here you can install packages with pip : (tutorial_env) ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ pip install pandas Installing setuptools and wheel projects are useful to ensure you can also install from source archives: (tutorial_env) ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ pip install --upgrade pip setuptools wheel (tutorial_env) ta5@rice-02:~$ pip freeze numpy==2.2.2 pandas==2.2.3 python-dateutil==2.9.0.post0 pytz==2024.2 setuptools==75.8.0 six==1.17.0 tzdata==2025.1 wheel==0.45.1 To deactivate or leave the environment tutorial_env : (tutorial_env) ta5@rice-02:~$ deactivate","title":"Virtual Environments"},{"location":"software/#virtual-environment-in-slurm","text":"Python virtual environments can be used in slurm jobs. To submit a sbatch job using a venv environment, you can source the environment at the top of the sbatch script. Sample python script that prints versions of packages: ta5@rice-02:~$ cat test.py import numpy as np import pandas as pd import sys print(f\"Python version = {sys.version}\") print(f\"Numpy version = {np.version.version}\") print(f\"Pandas version = {pd.__version__}\") To submit this script using the venv tutorial_env create a sbatch script to load the venv and run test.py : ta5@rice-02:~$ cat tutorial_env.sh #!/bin/bash #SBATCH --job-name=tutorial_env #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=1 #SBATCH --partition=normal # Load venv tutorial_env source tutorial_env/bin/activate # Run script python3 test.py ta5@rice-02:~$ ta5@rice-02:~$ ta5@rice-02:~$ sbatch tutorial_env.sh Submitted batch job 298438 ta5@rice-02:~$ cat slurm-298438.out Python version = 3.13.0 (main, Dec 10 2024, 13:22:44) [GCC 13.2.0] Numpy version = 2.2.2 Pandas version = 2.2.3","title":"Virtual Environment in Slurm"},{"location":"software/#jupyterlab","text":"JupyterLab is Project Jupyter\u2019s web-based development interface for Jupyter Notebooks. On FarmShare, it is available as an app on our OnDemand interface and supports computation with Python 3.","title":"JupyterLab"},{"location":"software/#accessing-jupyterlab","text":"Login in to OnDemand and select Interactive Apps > JupyterLab If you want to make one of your virtual environments available for use in Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment (if you do not have an environment, refer to the sections above on how to do so). ta5@rice-02:~$ source tutorial_env/bin/activate (tutorial_env) ta5@rice-02:~$ pip install ipykernel ta5@rice-02:~$ (tutorial_env) ta5@rice-02:~$ python3 -m ipykernel install --user --name tutorial_env Installed kernelspec tutorial_env in /home/users/ta5/.local/share/jupyter/kernels/tutorial_env Once you\u2019ve successfully created your kernel, you should see your environment (custom kernel name) at the Notebook Launcher!","title":"Accessing JupyterLab"},{"location":"software/#apptainer","text":"Containers are isolated environments packaged together with an executable so that no additional installation or setup is required for running on any system. Apptainer (formerly known as Singularity), is a container runtime that is available on FarmShare. Apptainer, and Linux containers in general allow sharing pipelines in a portable, reproducible way. You can create and customize your own containers, and because Apptainer also supports Docker containers, you have immediate access to a very large number of Apptainer and Docker containers available via repositories: DockerHub Sylabs","title":"Apptainer"},{"location":"software/#running-apptainer","text":"This example will request an interactive session and use the Docker container python/3.13.1-alpine3.21 from DockerHub. This container provides the latest release of python in an Alpine OS environment. The first step is to request an interactive session with multiple cores: ta5@rice-04:~$ srun --partition=interactive --cpus-per-task=4 --qos=interactive --pty bash ta5@iron-06:~$ Next create a directory /scratch/users/$USER/lxd to store all your images. Now load the apptainer module and pull the image: ta5@iron-06:~$ pwd /home/users/ta5 ta5@iron-06:~$ ta5@iron-06:~$ cd /scratch/users/$USER ta5@iron-06:/scratch/users/ta5$ ta5@iron-06:/scratch/users/ta5$ mkdir lxc ta5@iron-06:/scratch/users/ta5$ ta5@iron-06:/scratch/users/ta5$ cd lxc/ ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ module load apptainer ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ apptainer pull docker://python:3.13.1-alpine3.21 INFO: Converting OCI blobs to SIF format INFO: Starting build... Copying blob 2109cea89a77 done | Copying blob 1f3e46996e29 done | Copying blob b7c174cb6c8c done | Copying blob 7486ee1cd0b3 done | Copying config d5cb4e1bd6 done | Writing manifest to image destination 2025/01/29 12:52:06 info unpack layer: sha256:1f3e46996e2966e4faa5846e56e76e3748b7315e2ded61476c24403d592134f0 2025/01/29 12:52:06 info unpack layer: sha256:7486ee1cd0b33ed93151ce1d3f73254a0987b484773adb31f37fe42bad78ba63 2025/01/29 12:52:06 info unpack layer: sha256:b7c174cb6c8cb4276eaff5e9ebfb56eb7124be68c8fcb4518cb5eb6b18245cf5 2025/01/29 12:52:07 info unpack layer: sha256:2109cea89a773ab9365659a6a63a1b7d8be1f7be6031112a429533bd7ba07f68 INFO: Creating SIF file... ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ ls python_3.13.1-alpine3.21.sif ta5@iron-06:/scratch/users/ta5/lxc$ Once the image is downloaded, you can run the container using apptainer run : ta5@iron-06:/scratch/users/ta5/lxc$ apptainer run python_3.13.1-alpine3.21.sif Python 3.13.1 (main, Jan 24 2025, 19:30:15) [GCC 14.2.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> a = (1, 2, 3, 4, 5) >>> x = sum(a) >>> print(x) 15 >>> exit ta5@iron-06:/scratch/users/ta5/lxc$ To launch a shell within the container using apptainer shell : ta5@iron-06:/scratch/users/ta5/lxc$ apptainer shell python_3.13.1-alpine3.21.sif Apptainer> cat /etc/issue Welcome to Alpine Linux 3.21 Kernel \\r on an \\m (\\l) Apptainer> exit ta5@iron-06:/scratch/users/ta5/lxc$ ta5@iron-06:/scratch/users/ta5/lxc$ cat /etc/issue Ubuntu 22.04.5 LTS \\n \\l","title":"Running Apptainer"},{"location":"software/#apptainer-usage","text":"The most common apptainer commands are outlined in the following table. Command Description apptainer run <image> run predefined script within container apptainer exec <image> execute any command within container apptainer shell <image> run bash shell within container","title":"Apptainer usage"},{"location":"software/#batch-job-example","text":"In the example above Apptainer was running interactively. The example below shows how to run it as a batch job to calculates the sum of one to five: ta5@iron-03:/scratch/users/ta5/lxc$ cat sum.py a = (1, 2, 3, 4, 5) x = sum(a) print(x) To submit this script using apptainer, create a sbatch script to run apptainer exec : ta5@rice-03:/scratch/users/ta5/lxc$ cat ~/tutorial_lxc.sh #!/bin/bash #SBATCH --job-name=tutorial_lxc #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=4 #SBATCH --partition=normal # Load apptainer module load apptainer # Run script with apptainer exec apptainer exec python_3.13.1-alpine3.21.sif python sum.py ta5@rice-03:/scratch/users/ta5/lxc$ ta5@rice-03:/scratch/users/ta5/lxc$ sbatch ~/tutorial_lxc.sh Submitted batch job 299057 ta5@rice-03:/scratch/users/ta5/lxc$ cat slurm-299057.out 15","title":"Batch job example"}]}